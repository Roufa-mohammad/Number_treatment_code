{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06481eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Author:Roufa\n",
    "\n",
    "'''\n",
    "input: pdf file\n",
    "output: check numbering consistency in the book\n",
    "numbers 1-10 should be in spelled and 10+ should be in numericals. if they are not in that format then\n",
    "we return them as inconsistent.\n",
    "'''\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/copy_assessment_tool/modules_final/programmatic')\n",
    "import io\n",
    "import torch\n",
    "import pdfplumber\n",
    "from word2number import w2n\n",
    "import re\n",
    "import pdfplumber\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from extract_superscripts import is_super_Script_present\n",
    "from dummy_4_1 import extract_references_text_from_details\n",
    "from json_output import return_json_result\n",
    "\n",
    "\n",
    "\n",
    "def find_starting_page_TOC(pdf):\n",
    "\n",
    "    # get contents list till index\n",
    "    num_pages = len(pdf.pages)\n",
    "    (\n",
    "        # page_content,\n",
    "        page_content_dict,\n",
    "        common_text,\n",
    "        text_list_compare_prev,\n",
    "        content_page_flg,\n",
    "        prev_break_flg,\n",
    "        cur_break_flg,\n",
    "        content_page_cnt_flg,\n",
    "        content_page_cnt,\n",
    "        \n",
    "    ) = (\n",
    "        # [],\n",
    "        {},\n",
    "        [],\n",
    "        [],\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        True,\n",
    "        0,\n",
    "    \n",
    "    )\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf.pages[page_num]\n",
    "        text_data = page.within_bbox((0, 0, 550, 770)).extract_text_lines()\n",
    "        text = [td.get(\"text\") for td in text_data]\n",
    "        text = \"\\n\".join(text)\n",
    "        \n",
    "        text_dict = {str(page_num+1)+'~'+str(n)+\"~\"+td.get(\"text\"): td.get(\"x0\") for n,td in enumerate(text_data)}\n",
    " \n",
    "        \n",
    "        text_list_compare = re.sub(r\"[^a-zA-Z0-9 .:)\\]\\n]\", \"\", text)\n",
    "        text_list_compare = text_list_compare.split(\"\\n\")\n",
    "\n",
    "        text_list_compare = [\n",
    "            i.strip().lower() for i in text_list_compare if i.strip() != \"\"\n",
    "        ]\n",
    "        text_list_compare_prev.extend(text_list_compare)\n",
    "    \n",
    "        if (len(text_list_compare) > 0) and (((len(text_list_compare[0].split()) < 5)\n",
    "            and (\n",
    "                \"content\" in \"\".join(text_list_compare[0])\n",
    "                or \"index\" in \"\".join(text_list_compare[0])\n",
    "    \n",
    "            )) or content_page_flg):\n",
    "                    \n",
    "            if content_page_cnt_flg:\n",
    "                content_page_cnt += 1\n",
    "                \n",
    "            if prev_break_flg:\n",
    "                if (\n",
    "                    (len(text_list_compare) > 0)\n",
    "                    and (len(text_list_compare[0].split()) < 5)\n",
    "                    and (\n",
    "                        \"content\" in \"\".join(text_list_compare)\n",
    "                        or \"index\" in \"\".join(text_list_compare)\n",
    "\n",
    "                    )\n",
    "                ):\n",
    "                    cur_break_flg = True\n",
    "                    prev_break_flg = False\n",
    "                    content_page_cnt_flg = True\n",
    "                    content_page_cnt = 0\n",
    "                    page_content_dict.clear()\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            content_page_flg = True\n",
    "            page_content_dict.update(text_dict)\n",
    "            \n",
    "            num = page_num\n",
    "\n",
    "            for i in [\n",
    "                \"appendix\",\n",
    "                \"bibliography\",\n",
    "                \"index\",\n",
    "                \"list of\",\n",
    "                \"references\",\n",
    "                \"contributors\",\n",
    "            ]:\n",
    "                if ((fuzz.partial_ratio(i, text_list_compare[-1]) > 89) and (len(text_list_compare[-1].split()) < 4)) or (content_page_cnt > 10):\n",
    "                    prev_break_flg = True\n",
    "                    if content_page_cnt > 10:\n",
    "                        content_page_cnt_flg = False\n",
    "                    break\n",
    "            if prev_break_flg and cur_break_flg:\n",
    "                break\n",
    "        elif (len(text_list_compare) == 0) and content_page_flg:\n",
    "            break\n",
    "\n",
    "    return page_content_dict,page_num+1\n",
    "            \n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\"TahaDouaji/detr-doc-table-detection\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"TahaDouaji/detr-doc-table-detection\")\n",
    "\n",
    "def detr_table_boxes(image):\n",
    "    '''detr model to extract bounding box for bordered and boardedless tables'''\n",
    "    image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    final_bbox = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        final_bbox.append(box)\n",
    "\n",
    "\n",
    "    return final_bbox\n",
    "\n",
    "def not_within_bboxes(obj, bboxes):\n",
    "    \"\"\"Check if the object is in any of the table's bbox.\"\"\"\n",
    "    def obj_in_bbox(_bbox):\n",
    "\n",
    "        \n",
    "        v_mid = (obj[\"top\"] + obj[\"bottom\"]) / 2\n",
    "        h_mid = (obj[\"x0\"] + obj[\"x1\"]) / 2\n",
    "        x0, top, x1, bottom = _bbox\n",
    "        return (h_mid >= x0) and (h_mid < x1) and (v_mid >= top) and (v_mid < bottom)\n",
    "    return not any(obj_in_bbox(__bbox) for __bbox in bboxes)\n",
    "\n",
    "\n",
    "def img_cords_to_page_cords(boxes, img_width, pdf_width):\n",
    "    '''this code convert image coordinate table bounding box to pdf coordiante bounding box'''\n",
    "    x1, y1, x2, y2 = map(float, boxes)\n",
    "    \n",
    "    x1 = x1 / img_width * pdf_width\n",
    "    y1 = y1 / img_width * pdf_width\n",
    "    x2 = x2 / img_width * pdf_width\n",
    "    y2 = y2 / img_width * pdf_width\n",
    "    \n",
    "    return (x1,y1,x2,y2)\n",
    "\n",
    "\n",
    "##############\n",
    "def extract_number_spellings(sentence):\n",
    "    '''this function extractes the number and words of number  form sentence and using w2n library for words to spelling and regex for number'''\n",
    "    sentence = sentence.replace(\",\", \"\")\n",
    "    words = sentence.split(\" \")\n",
    "    spellings = []\n",
    "    for word in words:\n",
    "    \n",
    "        if word.isnumeric():\n",
    "            continue\n",
    "        try:\n",
    "            number = w2n.word_to_num(word)\n",
    "            if number >1:\n",
    "                if isinstance(number, int):\n",
    "                    new = word.split('\\n')[0]\n",
    "                    spellings.append(new)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    numerical_pattern = re.findall(r'\\b(?<!-)\\d+\\b(?!-)', sentence)\n",
    "  \n",
    "    return spellings, numerical_pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_for_consistency(combined):\n",
    "    '''\n",
    "    this take the extracted number then check if number range from 1-10 are string and afterwards numericals \n",
    "    '''\n",
    "    newlist_consistent = []\n",
    "    newlist_inconsistent = []\n",
    "    for word in combined:\n",
    "        try :\n",
    "            number = w2n.word_to_num(word)\n",
    "            if number < 10 and (word.isalpha() == True):  # non-integer & < 10\n",
    "                newlist_consistent.append(word)  # number\n",
    "            elif number > 10 and (word.isnumeric() == True):  # not strings & > 10\n",
    "                newlist_consistent.append(word)\n",
    "            elif number < 10 and (word.isnumeric() == True):\n",
    "                newlist_inconsistent.append(word)\n",
    "            elif number > 10 and (word.isalpha() == True):\n",
    "                newlist_inconsistent.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return newlist_consistent, newlist_inconsistent\n",
    "\n",
    "\n",
    "def process_document1(sentences):\n",
    "    '''this takes sentences as inputs and remove float point if it present any then call the function of extract_number_spellings and\n",
    "       check_for_consistency this check for consistancy function return the list of number which it is consistant and inconsistant\n",
    "    '''\n",
    "    \n",
    "    Check = []\n",
    "    # below pattern ro remove float point\n",
    "    pattern_to_remove = r'\\b\\d+(?:\\.\\d+)+\\b\\s*'\n",
    "    text_without_patterns = re.sub(pattern_to_remove, '', sentences)\n",
    "    spellings, numbers = extract_number_spellings(text_without_patterns)\n",
    "    combined = spellings + numbers\n",
    "\n",
    "    consistent_list, inconsistent_list = check_for_consistency(combined)\n",
    "    return consistent_list, inconsistent_list\n",
    "\n",
    "\n",
    "\n",
    "def num_treatment_dataframe(pdf_path,pdf,start_num,text_ref_dictionary):\n",
    "    '''\n",
    "    This fucntions find page has border or borderless table and removing the table,\n",
    "    this code takes each line pass though number-treatment code and return dataframe which is having pageno,line_no,inscosistant and there respective sentence\n",
    "    '''\n",
    "    sample_dict = {}\n",
    "\n",
    "    output_dict = {\n",
    "        \"page_no\": [],\n",
    "        \"line_nos\": [],\n",
    "        \"inconsistent\": [],\n",
    "        \"sentence\": []\n",
    "    }\n",
    "\n",
    "    for page_no, page in enumerate(pdf.pages):\n",
    "       \n",
    "        if page_no > start_num:\n",
    "            \n",
    "            if page.find_tables():\n",
    "                images = convert_from_path(pdf_path, first_page=page_no+1 , last_page=page_no+1 )\n",
    "                for j, img in enumerate(images):\n",
    "                    # Save the image as JPG and pass it to detr_table_boxes\n",
    "                    img_as_bytes = io.BytesIO()\n",
    "                    # Convert the current page to images\n",
    "\n",
    "                    img.save(img_as_bytes, format=\"JPEG\")\n",
    "                    img_as_bytes.seek(0)\n",
    "                    \n",
    "                    # Detect and get bounding boxes\n",
    "                    bboxes = detr_table_boxes(img_as_bytes)\n",
    "                    if bboxes:\n",
    "                        pdf_width  = page.width\n",
    "                        pdf_height = page.height\n",
    "                        image = Image.open(img_as_bytes).convert(\"RGB\")\n",
    "                        width,height = image.size\n",
    "                    \n",
    "\n",
    "                        convert_box =[]\n",
    "                        for boxes in bboxes:\n",
    "                            new = img_cords_to_page_cords(boxes, width, pdf_width)\n",
    "                            convert_box.append(new)\n",
    "\n",
    "                        for line_no, dictionary in enumerate(page.extract_text_lines()):\n",
    "                            del dictionary[\"chars\"]\n",
    "                            for page_cords in convert_box:\n",
    "                                if dictionary['top'] < int(page_cords[1]) or dictionary[\"top\"] > int(page_cords[3]):\n",
    "                                    sample_dict[line_no] = dictionary[\"text\"]\n",
    "                        val_text_if = text_ref_dictionary.get(page_no+1)\n",
    "                        for key,val_if in sample_dict.items():\n",
    "                            try:\n",
    "                                if val_if in val_text_if[\"text\"] :\n",
    "                                    \n",
    "                                        consistent_list,inconsistent_list=  process_document1(val_if)\n",
    "                                        # \n",
    "                                        if inconsistent_list:\n",
    "                                            output_dict[\"page_no\"].append(page_no+1)\n",
    "                                            output_dict[\"line_nos\"].append(key+1) #keys is a line_no\n",
    "                                            output_dict[\"inconsistent\"].append(inconsistent_list)\n",
    "                                            output_dict['sentence'].append(val_if)\n",
    "                            except:\n",
    "                                pass\n",
    "                                \n",
    "\n",
    "                    else:\n",
    "                        for line_no, dictionary in enumerate(page.extract_text_lines()):\n",
    "                            del dictionary[\"chars\"]\n",
    "                            sample_dict[line_no] = dictionary[\"text\"]\n",
    "                        val_text_else = text_ref_dictionary.get(page_no+1)\n",
    "                        for key,val_else in sample_dict.items():\n",
    "                            try:\n",
    "                                if val_else in val_text_else[\"text\"]:\n",
    "                                    \n",
    "                                \n",
    "                                        consistent_list,inconsistent_list=  process_document1(val_else)\n",
    "                                        if inconsistent_list:\n",
    "                                            output_dict[\"page_no\"].append(page_no+1)\n",
    "                                            output_dict[\"line_nos\"].append(key+1) #keys is a line_no\n",
    "                                            output_dict[\"inconsistent\"].append(inconsistent_list)\n",
    "                                            output_dict['sentence'].append(val_else)\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "                                \n",
    "                                          \n",
    "            else:\n",
    "                \n",
    "                for line_no, dictionary in enumerate(page.extract_text_lines()):\n",
    "                    del dictionary[\"chars\"]\n",
    "                    \n",
    "             \n",
    "                    sample_dict[line_no] = dictionary[\"text\"]\n",
    "\n",
    "                val_text = text_ref_dictionary.get(page_no+1)\n",
    "                for key,val in sample_dict.items():\n",
    "                    try:\n",
    "                        if val in val_text[\"text\"]:\n",
    "        \n",
    "                                consistent_list,inconsistent_list=  process_document1(val)\n",
    "                                if inconsistent_list:\n",
    "                                    output_dict[\"page_no\"].append(page_no+1)\n",
    "                                    output_dict[\"line_nos\"].append(key+1) #keys is a line_no\n",
    "                                    output_dict[\"inconsistent\"].append(inconsistent_list)\n",
    "                                    output_dict['sentence'].append(val)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                                    \n",
    "    return output_dict\n",
    "\n",
    "#if any superscript/footnotes in pdf then remove them as well\n",
    "def clean_line(line, results):\n",
    "    for res in results:\n",
    "        if res[0] in line['text']:\n",
    "            line['text'] = line['text'].replace(res[0], '')\n",
    "    return line\n",
    "\n",
    "def remove_references_from_doc(pdf, reference_details, super_script_results):\n",
    "    '''\n",
    "    This function remove the reference the text from the page and give remaining text\n",
    "    '''\n",
    "    start_page = 1\n",
    "    start_idx = 1\n",
    "    clean_lines_per_page = {}\n",
    "\n",
    "    for ref_detail in reference_details:\n",
    "        end_page = ref_detail['start_page_num']\n",
    "        end_idx = ref_detail['start_idx']\n",
    "\n",
    "\n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page = pdf.pages[page_num - 1]\n",
    "            lines = page.extract_text_lines()\n",
    "\n",
    "            if page_num == start_page:\n",
    "                lines = lines[start_idx - 1:]\n",
    "\n",
    "            if page_num == end_page:\n",
    "                lines = lines[:end_idx]\n",
    "            \n",
    "            cleaned_lines = []\n",
    "\n",
    "            for line in lines:\n",
    "                cleaned_line = clean_line(line, super_script_results)\n",
    "                cleaned_lines.append(cleaned_line['text'])\n",
    "            clean_lines_per_page[page_num] = {'text': cleaned_lines}\n",
    "\n",
    "\n",
    "        start_page = ref_detail['end_page_num']\n",
    "        start_idx = ref_detail['end_idx']\n",
    "\n",
    "    return clean_lines_per_page\n",
    "\n",
    "def final_num_reults_json(pdf,output_dict):\n",
    "    final_result = []\n",
    "    for index,page in enumerate(pdf.pages):\n",
    "        if index+1 in output_dict['page_no']:\n",
    "            lines_dict = page.extract_text_lines(return_chars=False)\n",
    "            for p_no,l_no,reslt in zip(output_dict[\"page_no\"], output_dict[\"line_nos\"], output_dict[\"inconsistent\"]):\n",
    "                if p_no == index+1:\n",
    "                    detail_dict = lines_dict[0]\n",
    "                    final_dict = {}\n",
    "                    final_dict[\"page\"] = p_no\n",
    "                    final_dict[\"text\"] = detail_dict[\"text\"]\n",
    "                    final_dict[\"bbox\"] = {\"x1\":detail_dict['x0'],\n",
    "                                            \"y1\":detail_dict['top'],\n",
    "                                            \"x2\":detail_dict['x1'],\n",
    "                                            \"y2\":detail_dict['bottom'],\n",
    "                                            \"width\":page.width,\n",
    "                                            \"height\":page.height\n",
    "                                            }\n",
    "                    final_dict[\"line_no\"] = l_no\n",
    "                    final_result.append(final_dict)\n",
    "                    \n",
    "    return final_result,len(output_dict[\"inconsistent\"])\n",
    "\n",
    "\n",
    "def complete_call(pdf, path):\n",
    "    super_script_results =  is_super_Script_present(pdf)\n",
    "    reference_details = extract_references_from_text(pdf)\n",
    "    clean_references_per_page = remove_references_from_doc(pdf, reference_details, super_script_results)\n",
    "    page_content_dict,page_num = find_starting_page_TOC(pdf)\n",
    "    output_dict = num_treatment_dataframe(path, pdf, page_num, clean_references_per_page)\n",
    "    final_result_num_dict, inconsitent_treatment_count = final_num_reults_json(pdf,output_dict)\n",
    "    json_numtreament = return_json_result(final_result_num_dict)\n",
    "    return inconsitent_treatment_count , json_numtreament\n",
    "\n",
    "# path = '/data/copy_assessment_tool/modules/data/15031-4988-FullBook.pdf'\n",
    "# pdf = pdfplumber.open(path)\n",
    "\n",
    "# inconsitent_treatment_count , json_numtreament = complete_call(pdf, path)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
